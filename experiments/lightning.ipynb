{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightning.pytorch as pl\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianglin/miniconda3/envs/genvis/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/xianglin/miniconda3/envs/genvis/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/xianglin/miniconda3/envs/genvis/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/xianglin/miniconda3/envs/genvis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xianglin/miniconda3/envs/genvis/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "2023-06-28 17:13:49.383871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 17:13:49.875749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from umap.umap_ import find_ab_params\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from singleVis.custom_weighted_random_sampler import CustomWeightedRandomSampler\n",
    "from singleVis.vis_models import vis_models as vmodels\n",
    "from singleVis.losses import UmapLoss, ReconstructionLoss, SingleVisLoss, LocalTemporalLoss, SmoothnessLoss\n",
    "from singleVis.edge_dataset import DVIDataHandler, LocalTemporalDataHandler\n",
    "from singleVis.trainer import SingleVisTrainer, LocalTemporalTrainer\n",
    "from singleVis.data import NormalDataProvider\n",
    "from singleVis.spatial_edge_constructor import LocalSpatialTemporalEdgeConstructor, SingleEpochSpatialEdgeConstructor\n",
    "from singleVis.projector import DVIProjector\n",
    "from singleVis.eval.evaluator import Evaluator\n",
    "from singleVis.visualizer import visualizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitInstanceNormAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, encoder_dims, decoder_dims, criterion):\n",
    "        super(LitInstanceNormAE, self).__init__()\n",
    "        self.criterion = criterion\n",
    "  \n",
    "        assert len(encoder_dims) > 1\n",
    "        assert len(decoder_dims) > 1\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.decoder_dims = decoder_dims\n",
    "\n",
    "        # Build Encoder\n",
    "        modules = list()\n",
    "        for i in range(0, len(self.encoder_dims)-2):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.encoder_dims[i], self.encoder_dims[i+1]),\n",
    "                nn.InstanceNorm1d(self.encoder_dims[i+1]),\n",
    "                nn.ReLU(True) \n",
    "                )\n",
    "            )\n",
    "        modules.append(nn.Linear(self.encoder_dims[-2], self.encoder_dims[-1]))\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = list()\n",
    "        for i in range(0, len(self.decoder_dims)-2):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(self.decoder_dims[i], self.decoder_dims[i+1]),\n",
    "                    nn.InstanceNorm1d(self.decoder_dims[i+1]),\n",
    "                    nn.ReLU(True)\n",
    "                )\n",
    "                \n",
    "            )\n",
    "        modules.append(nn.Linear(self.decoder_dims[-2], self.decoder_dims[-1]))\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, edge_to, edge_from):\n",
    "        outputs = dict()\n",
    "        embedding_to = self.encoder(edge_to)\n",
    "        embedding_from = self.encoder(edge_from)\n",
    "        recon_to = self.decoder(embedding_to)\n",
    "        recon_from = self.decoder(embedding_from)\n",
    "        \n",
    "        outputs[\"umap\"] = (embedding_to, embedding_from)\n",
    "        outputs[\"recon\"] = (recon_to, recon_from)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # edge_to, edge_from, a_to, a_from, coeffi_from, embedded_from = batch\n",
    "\n",
    "        # edge_to = edge_to.to(device=self.DEVICE, dtype=torch.float32)\n",
    "        # edge_from = edge_from.to(device=self.DEVICE, dtype=torch.float32)\n",
    "        # a_to = a_to.to(device=self.DEVICE, dtype=torch.float32)\n",
    "        # a_from = a_from.to(device=self.DEVICE, dtype=torch.float32)\n",
    "        # coeffi_from = coeffi_from.to(device=self.DEVICE, dtype=torch.bool)\n",
    "        # embedded_from = embedded_from.to(device=self.DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # outputs = self.model(edge_to, edge_from)\n",
    "        # _, _, _, loss = self.criterion(edge_to, edge_from, a_to, a_from, coeffi_from, embedded_from, outputs)\n",
    "        edge_to, edge_from, a_to, a_from = batch\n",
    "        edge_to = edge_to.to(dtype=torch.float32)\n",
    "        edge_from = edge_from.to(dtype=torch.float32)\n",
    "        a_to = a_to.to(dtype=torch.float32)\n",
    "        a_from = a_from.to(dtype=torch.float32)\n",
    "\n",
    "        outputs = self.forward(edge_to, edge_from)\n",
    "        _, _, loss = self.criterion(edge_to, edge_from, a_to, a_from, outputs)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Define training parameters\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=.01, weight_decay=1e-5)\n",
    "        # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, step_size=4, gamma=.1)\n",
    "        # return [optimizer], [lr_scheduler]\n",
    "        return [optimizer]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIS_METHOD = \"tDVI\"\n",
    "VIS_MODEL = 'litinAE'\n",
    "CONTENT_PATH = \"/home/xianglin/projects/DVI_data/resnet18_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(CONTENT_PATH)\n",
    "with open(os.path.join(CONTENT_PATH, \"config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = config[VIS_METHOD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTING = config[\"SETTING\"]\n",
    "CLASSES = config[\"CLASSES\"]\n",
    "DATASET = config[\"DATASET\"]\n",
    "PREPROCESS = config[\"VISUALIZATION\"][\"PREPROCESS\"]\n",
    "GPU_ID = config[\"GPU\"]\n",
    "EPOCH_START = config[\"EPOCH_START\"]\n",
    "EPOCH_END = config[\"EPOCH_END\"]\n",
    "EPOCH_PERIOD = config[\"EPOCH_PERIOD\"]\n",
    "EPOCH_NAME = config[\"EPOCH_NAME\"]\n",
    "\n",
    "# Training parameter (subject model)\n",
    "TRAINING_PARAMETER = config[\"TRAINING\"]\n",
    "NET = TRAINING_PARAMETER[\"NET\"]\n",
    "LEN = TRAINING_PARAMETER[\"train_num\"]\n",
    "\n",
    "# Training parameter (visualization model)\n",
    "VISUALIZATION_PARAMETER = config[\"VISUALIZATION\"]\n",
    "# VIS_MODEL = VISUALIZATION_PARAMETER[\"VIS_MODEL\"]\n",
    "LAMBDA1 = VISUALIZATION_PARAMETER[\"LAMBDA1\"]\n",
    "B_N_EPOCHS = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"B_N_EPOCHS\"]\n",
    "L_BOUND = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"L_BOUND\"]\n",
    "ENCODER_DIMS = VISUALIZATION_PARAMETER[\"ENCODER_DIMS\"]\n",
    "DECODER_DIMS = VISUALIZATION_PARAMETER[\"DECODER_DIMS\"]\n",
    "S_N_EPOCHS = VISUALIZATION_PARAMETER[\"S_N_EPOCHS\"]\n",
    "T_N_EPOCHS = VISUALIZATION_PARAMETER[\"T_N_EPOCHS\"]\n",
    "N_NEIGHBORS = VISUALIZATION_PARAMETER[\"N_NEIGHBORS\"]\n",
    "PATIENT = VISUALIZATION_PARAMETER[\"PATIENT\"]\n",
    "MAX_EPOCH = VISUALIZATION_PARAMETER[\"MAX_EPOCH\"]\n",
    "\n",
    "VIS_MODEL_NAME = VISUALIZATION_PARAMETER[\"VIS_MODEL_NAME\"]\n",
    "EVALUATION_NAME = VISUALIZATION_PARAMETER[\"EVALUATION_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "DEVICE = torch.device(\"cuda:{}\".format(GPU_ID) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import Model.model as subject_model\n",
    "net = eval(\"subject_model.{}()\".format(NET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initialization...\n"
     ]
    }
   ],
   "source": [
    "# Define data_provider\n",
    "data_provider = NormalDataProvider(CONTENT_PATH, net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, device=DEVICE, classes=CLASSES, epoch_name=EPOCH_NAME, verbose=1)\n",
    "if PREPROCESS:\n",
    "    data_provider._meta_data()\n",
    "    if B_N_EPOCHS >0:\n",
    "        data_provider._estimate_boundary(LEN//10, l_bound=L_BOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Losses\n",
    "negative_sample_rate = 5\n",
    "min_dist = .1\n",
    "_a, _b = find_ab_params(1.0, min_dist)\n",
    "umap_loss_fn = UmapLoss(negative_sample_rate, _a, _b, repulsion_strength=1.0)\n",
    "recon_loss_fn = ReconstructionLoss(beta=1.0)\n",
    "smooth_loss_fn = SmoothnessLoss(margin=0.0)\n",
    "# Define Criterion\n",
    "criterion = SingleVisLoss(umap_loss_fn, recon_loss_fn, lambd=LAMBDA1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define visualization models\n",
    "model = LitInstanceNormAE(ENCODER_DIMS, DECODER_DIMS, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Projector\n",
    "projector = DVIProjector(vis_model=model, content_path=CONTENT_PATH, vis_model_name=VIS_MODEL_NAME, epoch_name=EPOCH_NAME, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 28 17:23:11 2023 Building RP forest with 17 trees\n",
      "Wed Jun 28 17:23:12 2023 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\t 4  /  16\n",
      "\tStopping threshold met -- exiting after 4 iterations\n"
     ]
    }
   ],
   "source": [
    "# Define Edge dataset\n",
    "spatial_cons = SingleEpochSpatialEdgeConstructor(data_provider, EPOCH_START, S_N_EPOCHS, B_N_EPOCHS, N_NEIGHBORS, metric=\"euclidean\")\n",
    "edge_to, edge_from, probs, feature_vectors, attention = spatial_cons.construct()\n",
    "\n",
    "dataset = DVIDataHandler(edge_to, edge_from, feature_vectors, attention)\n",
    "\n",
    "n_samples = int(np.sum(S_N_EPOCHS * probs) // 1)\n",
    "# chose sampler based on the number of dataset\n",
    "if len(edge_to) > pow(2,24):\n",
    "    sampler = CustomWeightedRandomSampler(probs, n_samples, replacement=True)\n",
    "else:\n",
    "    sampler = WeightedRandomSampler(probs, n_samples, replacement=True)\n",
    "edge_loader = DataLoader(dataset, batch_size=1000, sampler=sampler, num_workers=4, prefetch_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | criterion | SingleVisLoss | 0     \n",
      "1 | encoder   | Sequential    | 174 K \n",
      "2 | decoder   | Sequential    | 175 K \n",
      "--------------------------------------------\n",
      "349 K     Trainable params\n",
      "0         Non-trainable params\n",
      "349 K     Total params\n",
      "1.399     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/379 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "# from lightning.pytorch import Trainer\n",
    "trainer = pl.Trainer(max_epochs=1, devices=[0], profiler='pytorch')\n",
    "trainer.fit(model=model, train_dataloaders=edge_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
