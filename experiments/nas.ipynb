{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from umap.umap_ import find_ab_params\n",
    "from dgl.geometry import farthest_point_sampler\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from singleVis.custom_weighted_random_sampler import CustomWeightedRandomSampler\n",
    "from singleVis.SingleVisualizationModel import VisModel\n",
    "from singleVis.losses import UmapLoss, ReconstructionLoss, SingleVisLoss\n",
    "from singleVis.edge_dataset import DataHandler\n",
    "from singleVis.data import NormalDataProvider\n",
    "from singleVis.spatial_edge_constructor import SingleEpochSpatialEdgeConstructor\n",
    "from singleVis.eval.evaluator import Evaluator\n",
    "from singleVis.projector import DVIProjector\n",
    "from singleVis.vis_models import vis_models as vmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIS_METHOD = \"singleDVI\" # DeepVisualInsight\n",
    "CONTENT_PATH = \"/home/xianglin/projects/DVI_data/resnet18_fmnist\"\n",
    "GPU_ID = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(CONTENT_PATH)\n",
    "with open(os.path.join(CONTENT_PATH, \"config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = config[VIS_METHOD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTING = config[\"SETTING\"]\n",
    "CLASSES = config[\"CLASSES\"]\n",
    "DATASET = config[\"DATASET\"]\n",
    "PREPROCESS = config[\"VISUALIZATION\"][\"PREPROCESS\"]\n",
    "\n",
    "# Training parameter (subject model)\n",
    "TRAINING_PARAMETER = config[\"TRAINING\"]\n",
    "NET = TRAINING_PARAMETER[\"NET\"]\n",
    "LEN = TRAINING_PARAMETER[\"train_num\"]\n",
    "EPOCH_START = config[\"EPOCH_START\"]\n",
    "EPOCH_END = config[\"EPOCH_END\"]\n",
    "EPOCH_PERIOD = config[\"EPOCH_PERIOD\"]\n",
    "\n",
    "# Training parameter (visualization model)\n",
    "VISUALIZATION_PARAMETER = config[\"VISUALIZATION\"]\n",
    "VIS_MODEL = VISUALIZATION_PARAMETER['VIS_MODEL']\n",
    "LAMBDA = VISUALIZATION_PARAMETER[\"LAMBDA\"]\n",
    "B_N_EPOCHS = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"B_N_EPOCHS\"]\n",
    "L_BOUND = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"L_BOUND\"]\n",
    "ENCODER_DIMS = VISUALIZATION_PARAMETER[\"ENCODER_DIMS\"]\n",
    "DECODER_DIMS = VISUALIZATION_PARAMETER[\"DECODER_DIMS\"]\n",
    "S_N_EPOCHS = VISUALIZATION_PARAMETER[\"S_N_EPOCHS\"]\n",
    "N_NEIGHBORS = VISUALIZATION_PARAMETER[\"N_NEIGHBORS\"]\n",
    "PATIENT = VISUALIZATION_PARAMETER[\"PATIENT\"]\n",
    "MAX_EPOCH = VISUALIZATION_PARAMETER[\"MAX_EPOCH\"]\n",
    "\n",
    "VIS_MODEL_NAME = VISUALIZATION_PARAMETER[\"VIS_MODEL_NAME\"]\n",
    "EVALUATION_NAME = VISUALIZATION_PARAMETER[\"EVALUATION_NAME\"]\n",
    "\n",
    "# Define hyperparameters\n",
    "DEVICE = torch.device(\"cuda:{}\".format(GPU_ID) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model.model as subject_model\n",
    "net = eval(\"subject_model.{}()\".format(NET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data_provider\n",
    "data_provider = NormalDataProvider(CONTENT_PATH, net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, device=DEVICE, classes=CLASSES, epoch_name=\"Epoch\", verbose=1)\n",
    "if PREPROCESS:\n",
    "    data_provider._meta_data()\n",
    "    if B_N_EPOCHS >0:\n",
    "        data_provider._estimate_boundary(LEN//10, l_bound=L_BOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define visualization models\n",
    "model = vmodels[VIS_MODEL](ENCODER_DIMS, DECODER_DIMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try nas metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def get_layer_metric_array(network, metric, mode): \n",
    "    metric_array = []\n",
    "    for layer in network.modules():\n",
    "        if mode=='channel' and hasattr(layer,'dont_ch_prune'):\n",
    "            continue\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            metric_array.append(metric(layer))\n",
    "    \n",
    "    return metric_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm(layer):\n",
    "    if layer.weight.grad is not None:\n",
    "        return layer.weight.grad\n",
    "    else:\n",
    "        return torch.zeros_like(layer.weight)\n",
    "def get_grad_norm_arr(network, data, loss_fn):\n",
    "    network.to(device=DEVICE)\n",
    "    network.train()\n",
    "    network.zero_grad()\n",
    "\n",
    "    # for data in self.edge_loader:\\\n",
    "    edge_to, edge_from, a_to, a_from = data\n",
    "\n",
    "    edge_to = edge_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    edge_from = edge_from.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_to = a_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_from = a_from.to(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    outputs = network(edge_to, edge_from)\n",
    "    _, _, loss = loss_fn(edge_to, edge_from, a_to, a_from, outputs)\n",
    "    # ===================backward====================\n",
    "    loss.backward()\n",
    "    grad_norm_arr = get_layer_metric_array(network, grad_norm, mode='param')\n",
    "\n",
    "    return grad_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm_array(network):\n",
    "    return get_layer_metric_array(network, lambda l: l.weight, mode=\"param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snip\n",
    "import types\n",
    "import torch.nn.functional as F\n",
    "def snip_forward_conv2d(self, x):\n",
    "    return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
    "                    self.stride, self.padding, self.dilation, self.groups)\n",
    "def snip_forward_linear(self, x):\n",
    "    return F.linear(x, self.weight * self.weight_mask, self.bias)\n",
    "\n",
    "def compute_snip_per_weight(network, data, loss_fn):\n",
    "    for layer in network.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "            layer.weight.requires_grad = False\n",
    "\n",
    "        # Override the forward methods:\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
    "\n",
    "    # Compute gradients (but don't apply them)\n",
    "    network.to(device=DEVICE)\n",
    "    network.train()\n",
    "    network.zero_grad()\n",
    "\n",
    "    # for data in self.edge_loader:\\\n",
    "    edge_to, edge_from, a_to, a_from = data\n",
    "\n",
    "    edge_to = edge_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    edge_from = edge_from.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_to = a_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_from = a_from.to(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    outputs = network(edge_to, edge_from)\n",
    "    _, _, loss = loss_fn(edge_to, edge_from, a_to, a_from, outputs)\n",
    "    # ===================backward====================\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # select the gradients that we want to use for search/prune\n",
    "    def snip(layer):\n",
    "        if layer.weight_mask.grad is not None:\n",
    "            return torch.abs(layer.weight_mask.grad)\n",
    "        else:\n",
    "            return torch.zeros_like(layer.weight)\n",
    "    \n",
    "    grads_abs = get_layer_metric_array(network, snip, mode=\"param\")\n",
    "\n",
    "    return grads_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "def compute_grasp_per_weight(network, data, loss_fn):\n",
    "\n",
    "    # get all applicable weights\n",
    "    weights = []\n",
    "    for layer in network.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            weights.append(layer.weight)\n",
    "            layer.weight.requires_grad_(True) # TODO isn't this already true?\n",
    "\n",
    "    # NOTE original code had some input/target splitting into 2\n",
    "    # I am guessing this was because of GPU mem limit\n",
    "    network.to(device=DEVICE)\n",
    "    network.train()\n",
    "    network.zero_grad()\n",
    "\n",
    "    #forward/grad pass #1\n",
    "    grad_w = None\n",
    "    #TODO get new data, otherwise num_iters is useless!\n",
    "    # for data in self.edge_loader:\\\n",
    "    edge_to, edge_from, a_to, a_from = data\n",
    "\n",
    "    edge_to = edge_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    edge_from = edge_from.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_to = a_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_from = a_from.to(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    outputs = network(edge_to, edge_from)\n",
    "    _, _, loss = loss_fn(edge_to, edge_from, a_to, a_from, outputs)\n",
    "    \n",
    "    grad_w_p = autograd.grad(loss, weights, allow_unused=True)\n",
    "    if grad_w is None:\n",
    "        grad_w = list(grad_w_p)\n",
    "    else:\n",
    "        for idx in range(len(grad_w)):\n",
    "            grad_w[idx] += grad_w_p[idx]\n",
    "\n",
    "\n",
    "    # forward/grad pass #2\n",
    "    edge_to, edge_from, a_to, a_from = data\n",
    "\n",
    "    edge_to = edge_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    edge_from = edge_from.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_to = a_to.to(device=DEVICE, dtype=torch.float32)\n",
    "    a_from = a_from.to(device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    outputs = network(edge_to, edge_from)\n",
    "    _, _, loss = loss_fn(edge_to, edge_from, a_to, a_from, outputs)\n",
    "    \n",
    "    grad_f = autograd.grad(loss, weights, create_graph=True, allow_unused=True)\n",
    "    \n",
    "    # accumulate gradients computed in previous step and call backwards\n",
    "    z, count = 0,0\n",
    "    for layer in network.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if grad_w[count] is not None:\n",
    "                z += (grad_w[count].data * grad_f[count]).sum()\n",
    "            count += 1\n",
    "    z.backward()\n",
    "\n",
    "    # compute final sensitivity metric and put in grads\n",
    "    def grasp(layer):\n",
    "        if layer.weight.grad is not None:\n",
    "            return -layer.weight.data * layer.weight.grad   # -theta_q Hg\n",
    "            #NOTE in the grasp code they take the *bottom* (1-p)% of values\n",
    "            #but we take the *top* (1-p)%, therefore we remove the -ve sign\n",
    "            #EDIT accuracy seems to be negatively correlated with this metric, so we add -ve sign here!\n",
    "        else:\n",
    "            return torch.zeros_like(layer.weight)\n",
    "    \n",
    "    grads = get_layer_metric_array(network, grasp, mode=\"param\")\n",
    "\n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(arr):\n",
    "    s = 0.0\n",
    "    for i in arr:\n",
    "        s += i.sum().item()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisModel([512,256,2], [2,256,512])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutual info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_data= data_provider.train_representation(1)\n",
    "# n_data = data_provider.train_representation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pynndescent import NNDescent\n",
    "# index = NNDescent(p_data, metric=\"cosine\")\n",
    "# neighbors = index.query(n_data, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = neighbors[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist.max(), dist.mean(), dist.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 200\n",
    "ratio = .1\n",
    "strategy = \"random\" # \"random\"\n",
    "# 200->0.125\n",
    "# 100->0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Losses\n",
    "negative_sample_rate = 5\n",
    "min_dist = .1\n",
    "_a, _b = find_ab_params(1.0, min_dist)\n",
    "umap_loss_fn = UmapLoss(negative_sample_rate, _a, _b, repulsion_strength=1.0)\n",
    "recon_loss_fn = ReconstructionLoss(beta=1.0)\n",
    "\n",
    "# Define DVI Loss\n",
    "criterion = SingleVisLoss(umap_loss_fn, recon_loss_fn, lambd=LAMBDA1)\n",
    "\n",
    "# Define training parameters\n",
    "# Define Edge dataset\n",
    "spatial_cons = SingleEpochSpatialEdgeConstructor(data_provider, I, S_N_EPOCHS, B_N_EPOCHS, N_NEIGHBORS, metric=\"cosine\")\n",
    "train_data = data_provider.train_representation(I)\n",
    "if strategy == \"fps\":\n",
    "    # farthest point sampling\n",
    "    data = torch.from_numpy(train_data[np.newaxis,:,:]).to(device=torch.device(\"cuda:1\"))\n",
    "    point_idxs = farthest_point_sampler(data, int(ratio*len(train_data)))\n",
    "    point_idxs = point_idxs.cpu().numpy().squeeze(0)\n",
    "    train_data = train_data[point_idxs]\n",
    "else:\n",
    "    # random sampling\n",
    "    selected = np.random.choice(len(train_data), int(ratio*len(train_data)), replace=False)\n",
    "    train_data = train_data[selected]\n",
    "edge_to, edge_from, probs, feature_vectors, attention = spatial_cons.construct(train_data)\n",
    "\n",
    "probs = probs / (probs.max()+1e-3)\n",
    "eliminate_zeros = probs>1e-2#1e-3\n",
    "edge_to = edge_to[eliminate_zeros]\n",
    "edge_from = edge_from[eliminate_zeros]\n",
    "probs = probs[eliminate_zeros]\n",
    "\n",
    "dataset = DataHandler(edge_to, edge_from, feature_vectors, attention)\n",
    "\n",
    "n_samples = int(np.sum(S_N_EPOCHS * probs) // 1)\n",
    "# chose sampler based on the number of dataset\n",
    "if len(edge_to) > pow(2,24):\n",
    "    sampler = CustomWeightedRandomSampler(probs, n_samples, replacement=True)\n",
    "else:\n",
    "    sampler = WeightedRandomSampler(probs, n_samples, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_loader = DataLoader(dataset, batch_size=1000, sampler=sampler)\n",
    "print(I,  \"All\")\n",
    "grasp_avg = 0\n",
    "snip_avg = 0\n",
    "c = 0\n",
    "for data in edge_loader:\n",
    "    c += 1 \n",
    "    grasp_avg += get_sum(compute_grasp_per_weight(model, data, criterion))\n",
    "    # print(f'grasp ratio:\\t{ratio}\\t', grasp)\n",
    "    snip_avg += get_sum(compute_snip_per_weight(model, data, criterion))\n",
    "    # print(f'snip ratio:\\t{ratio}\\t', snip_grad)\n",
    "print(f'grasp ratio:\\t{ratio}\\t', grasp_avg/c)\n",
    "print(f'snip ratio:\\t{ratio}\\t', snip_avg/c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I, strategy)\n",
    "edge_loader = DataLoader(dataset, batch_size=1000, sampler=sampler)\n",
    "grasp_avg = 0\n",
    "snip_avg = 0\n",
    "c = 0\n",
    "for data in edge_loader:\n",
    "    c += 1 \n",
    "    grasp_avg += get_sum(compute_grasp_per_weight(model, data, criterion))\n",
    "    # print(f'grasp ratio:\\t{ratio}\\t', grasp)\n",
    "    snip_avg += get_sum(compute_snip_per_weight(model, data, criterion))\n",
    "    # print(f'snip ratio:\\t{ratio}\\t', snip_grad)\n",
    "print(f'grasp ratio:\\t{ratio}\\t', grasp_avg/c)\n",
    "print(f'snip ratio:\\t{ratio}\\t', snip_avg/c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two core problems:\n",
    "1. decide ratio\n",
    "2. decide what select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from singleVis.utils import knn\n",
    "def density_estimation(train_data, k=50, metric=\"euclidean\"):\n",
    "    _, knn_dists = knn(train_data, k, metric)\n",
    "    avg_distance = knn_dists[:, -1]\n",
    "    density = k / avg_distance\n",
    "    return avg_distance.mean(), density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ratio\n",
    "whether the boundary and the gap between two nearest samples are comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess: density drop drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute boundary dist\n",
    "\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "train_data = data_provider.train_representation(I)\n",
    "labels = data_provider.train_labels(I)\n",
    "boundaries_dists = np.zeros((10,10))\n",
    "t0 = time.time()\n",
    "for s in range(10):\n",
    "    for e in range(s+1, 10):\n",
    "        cls_1 = train_data[labels==s]\n",
    "        cls_2 = train_data[labels==e]\n",
    "        n_neighbors = 10\n",
    "        high_neigh = NearestNeighbors(n_neighbors=n_neighbors, metric=\"euclidean\")\n",
    "        high_neigh.fit(cls_1)\n",
    "        dists_1, _ = high_neigh.kneighbors(cls_2, n_neighbors=n_neighbors, return_distance=True)\n",
    "        boundaries_dists[s][e] = dists_1[:, -1].mean()\n",
    "t1 = time.time()\n",
    "print(f\"Time Taken: {t1-t0:.2f}\")\n",
    "boundaries_dists = boundaries_dists+boundaries_dists.transpose()\n",
    "boundaries_dists = boundaries_dists+np.eye(10)*boundaries_dists.max()\n",
    "b_min = boundaries_dists.min()\n",
    "b_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_provider.train_representation(I)\n",
    "avg_distance, density = density_estimation(train_data)\n",
    "avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [0.0008, 0.001, 0.002, 0.003, 0.05, 0.1, 0.2,0.3,0.4,0.5, 0.6,0.7, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accus_train = list()\n",
    "accus_test = list()\n",
    "ppr_train = list()\n",
    "ppr_test = list()\n",
    "for ratio in ratios:\n",
    "    scripts = os.path.join(CONTENT_PATH, \"Model\", f\"evaluation_singleDVI_{VIS_MODEL}_{ratio}.json\")\n",
    "    with open(scripts, \"r\") as f:\n",
    "        eval = json.load(f)\n",
    "    accus_train.append(eval[\"nn_train\"][str(I)][\"15\"])\n",
    "    accus_test.append(eval[\"nn_test\"][str(I)][\"15\"])\n",
    "    ppr_train.append(eval[\"ppr_train\"][str(I)])\n",
    "    ppr_test.append(eval[\"ppr_test\"][str(I)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interpolation function\n",
    "f1 = interp1d(ratios, accus_train, kind=\"linear\")\n",
    "f2 = interp1d(ratios, accus_test, kind=\"linear\")\n",
    "f3 = interp1d(ratios, ppr_train, kind=\"linear\")\n",
    "f4 = interp1d(ratios, ppr_test, kind=\"linear\")\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1, ncols=2, sharex=True,figsize=(12, 6))\n",
    "\n",
    "# Create a new x array with more points for a smoother plot\n",
    "xnew = np.linspace(0.0008, 1.0, 100)\n",
    "ax[0].set_title('nn')\n",
    "ax[0].plot(xnew, f1(xnew),label=\"train\")\n",
    "ax[0].plot(xnew, f2(xnew),label=\"test\")\n",
    "ax[0].set_ylim(0, max(accus_train)+0.2)\n",
    "\n",
    "ax[1].set_title('ppr')\n",
    "ax[1].plot(xnew, f3(xnew),label=\"train\")\n",
    "ax[1].plot(xnew, f4(xnew),label=\"test\")\n",
    "\n",
    "fig.suptitle(f'I={I}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.4\n",
    "repeat = 2\n",
    "m = np.zeros(repeat)\n",
    "train_data = data_provider.train_representation(I)\n",
    "# random sampling\n",
    "for i in range(repeat):\n",
    "    selected = np.random.choice(len(train_data), int(ratio*len(train_data)), replace=False)\n",
    "    avg_distance_, density_tmp = density_estimation(train_data[selected])\n",
    "    m[i] = avg_distance_\n",
    "m.mean(), m.mean()/avg_distance, m.mean()/b_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## density-aware selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Projector\n",
    "ratio=1.0\n",
    "# VIS_MODEL_NAME = f\"{VIS_MODEL_NAME}_{ratio}\"\n",
    "projector = DVIProjector(vis_model=model, content_path=CONTENT_PATH, vis_model_name=f\"{VIS_MODEL_NAME}_{ratio}\", epoch_name=\"Epoch\", device=DEVICE)\n",
    "evaluator = Evaluator(data_provider, projector, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = evaluator.eval_nn_train(I, 15)\n",
    "train_data = data_provider.train_representation(I)\n",
    "conf = data_provider.get_pred(I, train_data)\n",
    "from scipy.special import softmax\n",
    "conf = softmax(conf, axis=1)\n",
    "conf = conf.max(axis=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "res = stats.spearmanr(nn, density)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.spearmanr(nn, conf)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.spearmanr(density, conf)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.spearmanr(nn, conf*density)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "N, bins, patches = ax.hist(density, edgecolor='white', linewidth=1, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from jenkspy import JenksNaturalBreaks\n",
    "\n",
    "jnb = JenksNaturalBreaks(10)\n",
    "jnb.fit(density)\n",
    "breaks = jnb.breaks_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "N, bins, patches = ax.hist(density, edgecolor='white', linewidth=1, bins=100)\n",
    "for i in range(len(breaks)):\n",
    "    ax.scatter([breaks[i]],[100], c='black',s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(density, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(density, nn, s=0.1, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_provider.train_representation(I)\n",
    "embedding = projector.batch_project(I, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = (nn>7)\n",
    "plt.scatter(embedding[:,0][selected], embedding[:,1][selected], c=density[selected], s=0.1, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:,0], embedding[:,1], c=density, s=0.1, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:,0], embedding[:,1], c=labels, cmap=\"tab20c\", s=0.1, alpha=0.8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('SV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa7a9f36e1a1e240450dbe9cc8f6d8df1d5301f36681fb271c44fdd883236b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
